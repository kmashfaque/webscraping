{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60acbad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "PATH=\"C:\\\\Users\\\\jashfaque\\\\Desktop\\\\dashboardSoft\\\\chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfc5ddc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e9a71d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a39909cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47294c55",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=123.0.6312.58)\nStacktrace:\n\tGetHandleVerifier [0x00007FF7D7767062+63090]\n\t(No symbol) [0x00007FF7D76D2CB2]\n\t(No symbol) [0x00007FF7D756EC65]\n\t(No symbol) [0x00007FF7D754CA7C]\n\t(No symbol) [0x00007FF7D75DD687]\n\t(No symbol) [0x00007FF7D75F2AC1]\n\t(No symbol) [0x00007FF7D75D6D83]\n\t(No symbol) [0x00007FF7D75A83A8]\n\t(No symbol) [0x00007FF7D75A9441]\n\tGetHandleVerifier [0x00007FF7D7B625CD+4238301]\n\tGetHandleVerifier [0x00007FF7D7B9F72D+4488509]\n\tGetHandleVerifier [0x00007FF7D7B97A0F+4456479]\n\tGetHandleVerifier [0x00007FF7D78405A6+953270]\n\t(No symbol) [0x00007FF7D76DE57F]\n\t(No symbol) [0x00007FF7D76D9254]\n\t(No symbol) [0x00007FF7D76D938B]\n\t(No symbol) [0x00007FF7D76C9BC4]\n\tBaseThreadInitThunk [0x00007FFD06177344+20]\n\tRtlUserThreadStart [0x00007FFD070626B1+33]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m wait \u001b[38;5;241m=\u001b[39m WebDriverWait(driver, \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Find and select the input field with ID \"txtfromdate\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m input_field \u001b[38;5;241m=\u001b[39m wait\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39melement_to_be_clickable((By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//input[@id=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxtfromdate\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m     18\u001b[0m input_field\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Find and click the button with ID \"btnFrmS1\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:96\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m         value \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver)\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value:\n\u001b[0;32m     98\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\support\\expected_conditions.py:363\u001b[0m, in \u001b[0;36melement_to_be_clickable.<locals>._predicate\u001b[1;34m(driver)\u001b[0m\n\u001b[0;32m    361\u001b[0m target \u001b[38;5;241m=\u001b[39m mark\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, WebElement):  \u001b[38;5;66;03m# if given locator instead of WebElement\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     target \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(\u001b[38;5;241m*\u001b[39mtarget)  \u001b[38;5;66;03m# grab element at locator\u001b[39;00m\n\u001b[0;32m    364\u001b[0m element \u001b[38;5;241m=\u001b[39m visibility_of(target)(driver)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m element \u001b[38;5;129;01mand\u001b[39;00m element\u001b[38;5;241m.\u001b[39mis_enabled():\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:741\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    738\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    739\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mFIND_ELEMENT, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m\"\u001b[39m: by, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: value})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=123.0.6312.58)\nStacktrace:\n\tGetHandleVerifier [0x00007FF7D7767062+63090]\n\t(No symbol) [0x00007FF7D76D2CB2]\n\t(No symbol) [0x00007FF7D756EC65]\n\t(No symbol) [0x00007FF7D754CA7C]\n\t(No symbol) [0x00007FF7D75DD687]\n\t(No symbol) [0x00007FF7D75F2AC1]\n\t(No symbol) [0x00007FF7D75D6D83]\n\t(No symbol) [0x00007FF7D75A83A8]\n\t(No symbol) [0x00007FF7D75A9441]\n\tGetHandleVerifier [0x00007FF7D7B625CD+4238301]\n\tGetHandleVerifier [0x00007FF7D7B9F72D+4488509]\n\tGetHandleVerifier [0x00007FF7D7B97A0F+4456479]\n\tGetHandleVerifier [0x00007FF7D78405A6+953270]\n\t(No symbol) [0x00007FF7D76DE57F]\n\t(No symbol) [0x00007FF7D76D9254]\n\t(No symbol) [0x00007FF7D76D938B]\n\t(No symbol) [0x00007FF7D76C9BC4]\n\tBaseThreadInitThunk [0x00007FFD06177344+20]\n\tRtlUserThreadStart [0x00007FFD070626B1+33]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"http://192.168.168.216/premierJute/myPanel/userdefinedreports\")\n",
    "\n",
    "wait = WebDriverWait(driver, 60)\n",
    "\n",
    "# Find and select the input field with ID \"txtfromdate\"\n",
    "input_field = wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='txtfromdate']\")))\n",
    "input_field.click()\n",
    "\n",
    "\n",
    "# Find and click the button with ID \"btnFrmS1\"\n",
    "button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@id='btnFrmS1']\")))\n",
    "driver.execute_script(\"arguments[0].click();\", button)\n",
    "\n",
    "\n",
    "# Find and click the input field with ID \"txttodate\"\n",
    "input_field = wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='txttodate']\")))\n",
    "input_field.click()\n",
    "\n",
    "# Find and click the button with ID \"btnToS3\"\n",
    "button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@id='btnToS1']\")))\n",
    "button.click()\n",
    "\n",
    "# Wait for a brief period before clicking the next button\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Click on the dropdown\n",
    "dropdown = wait.until(EC.element_to_be_clickable((By.ID, \"select2-body_ddlreportemp-container\")))\n",
    "dropdown.click()\n",
    "\n",
    "\n",
    "# Locate and click the \"Breakage\" option\n",
    "manager_option = wait.until(EC.element_to_be_clickable((By.XPATH, \"//li[contains(text(), 'Breakage')]\")))\n",
    "manager_option.click()\n",
    "\n",
    "# Find the \"Load\" button element\n",
    "load_button_1 = wait.until(EC.element_to_be_clickable((By.ID, \"Btnsearchid\")))\n",
    "\n",
    "# Click the \"Load\" button\n",
    "load_button_1.click()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "html_code_br = driver.page_source\n",
    "# Parse the HTML content\n",
    "\n",
    "soup_br = BeautifulSoup(html_code_br, 'html.parser')\n",
    "\n",
    "# Find the table element\n",
    "table_br = soup_br.find('table', {'id': 'usertable'})  # Replace 'tblline' with the actual ID of your table\n",
    "\n",
    "# Extract the table headers\n",
    "headers_br = [th.text.strip() for th in table_br.select('thead th')]\n",
    "\n",
    "# Extract the table rows\n",
    "rows_br = []\n",
    "for tr in table_br.select('tbody tr'):\n",
    "    row_br = [td.text.strip() for td in tr.select('td')]\n",
    "    rows_br.append(row_br)\n",
    "\n",
    "# Check if there are any rows in the footer of the table\n",
    "footer_rows_br = table_br.select('tfoot tr')\n",
    "if footer_rows_br:\n",
    "    # Extract the additional rows from the footer\n",
    "    additional_rows_br = []\n",
    "    for tr in footer_rows_br:\n",
    "        row_br = [td.text.strip() for td in tr.select('td')]\n",
    "        additional_rows_br.append(row_br)\n",
    "\n",
    "    # Add the additional rows at the end of the extracted rows\n",
    "    rows_br.extend(additional_rows_br)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df_br = pd.DataFrame(rows_br, columns=headers_br)\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "df_br.to_excel('breakage_A.xlsx', index=False)\n",
    "\n",
    "\n",
    "\n",
    "time.sleep(10)\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2110b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"http://192.168.168.216/premierJute/myPanel/userdefinedreports\")\n",
    "\n",
    "wait = WebDriverWait(driver, 60)\n",
    "\n",
    "# Find and select the input field with ID \"txtfromdate\"\n",
    "input_field = wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='txtfromdate']\")))\n",
    "input_field.click()\n",
    "\n",
    "\n",
    "# Find and click the button with ID \"btnFrmS1\"\n",
    "button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@id='btnFrmS2']\")))\n",
    "driver.execute_script(\"arguments[0].click();\", button)\n",
    "\n",
    "\n",
    "# Find and click the input field with ID \"txttodate\"\n",
    "input_field = wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='txttodate']\")))\n",
    "input_field.click()\n",
    "\n",
    "# Find and click the button with ID \"btnToS3\"\n",
    "button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@id='btnToS2']\")))\n",
    "button.click()\n",
    "\n",
    "# Wait for a brief period before clicking the next button\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Click on the dropdown\n",
    "dropdown = wait.until(EC.element_to_be_clickable((By.ID, \"select2-body_ddlreportemp-container\")))\n",
    "dropdown.click()\n",
    "\n",
    "\n",
    "# Locate and click the \"Breakage\" option\n",
    "manager_option = wait.until(EC.element_to_be_clickable((By.XPATH, \"//li[contains(text(), 'Breakage')]\")))\n",
    "manager_option.click()\n",
    "\n",
    "# Find the \"Load\" button element\n",
    "load_button_1 = wait.until(EC.element_to_be_clickable((By.ID, \"Btnsearchid\")))\n",
    "\n",
    "# Click the \"Load\" button\n",
    "load_button_1.click()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "html_code_br = driver.page_source\n",
    "# Parse the HTML content\n",
    "\n",
    "soup_br = BeautifulSoup(html_code_br, 'html.parser')\n",
    "\n",
    "# Find the table element\n",
    "table_br = soup_br.find('table', {'id': 'usertable'})  # Replace 'tblline' with the actual ID of your table\n",
    "\n",
    "# Extract the table headers\n",
    "headers_br = [th.text.strip() for th in table_br.select('thead th')]\n",
    "\n",
    "# Extract the table rows\n",
    "rows_br = []\n",
    "for tr in table_br.select('tbody tr'):\n",
    "    row_br = [td.text.strip() for td in tr.select('td')]\n",
    "    rows_br.append(row_br)\n",
    "\n",
    "# Check if there are any rows in the footer of the table\n",
    "footer_rows_br = table_br.select('tfoot tr')\n",
    "if footer_rows_br:\n",
    "    # Extract the additional rows from the footer\n",
    "    additional_rows_br = []\n",
    "    for tr in footer_rows_br:\n",
    "        row_br = [td.text.strip() for td in tr.select('td')]\n",
    "        additional_rows_br.append(row_br)\n",
    "\n",
    "    # Add the additional rows at the end of the extracted rows\n",
    "    rows_br.extend(additional_rows_br)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df_br = pd.DataFrame(rows_br, columns=headers_br)\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "df_br.to_excel('breakage_B.xlsx', index=False)\n",
    "\n",
    "\n",
    "\n",
    "time.sleep(10)\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0499b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"http://192.168.168.216/premierJute/myPanel/userdefinedreports\")\n",
    "\n",
    "wait = WebDriverWait(driver, 60)\n",
    "\n",
    "# Find and select the input field with ID \"txtfromdate\"\n",
    "input_field = wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='txtfromdate']\")))\n",
    "input_field.click()\n",
    "\n",
    "\n",
    "# Find and click the button with ID \"btnFrmS1\"\n",
    "button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@id='btnFrmS3']\")))\n",
    "driver.execute_script(\"arguments[0].click();\", button)\n",
    "\n",
    "\n",
    "# Find and click the input field with ID \"txttodate\"\n",
    "input_field = wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='txttodate']\")))\n",
    "input_field.click()\n",
    "\n",
    "# Find and click the button with ID \"btnToS3\"\n",
    "button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@id='btnToS3']\")))\n",
    "button.click()\n",
    "\n",
    "# Wait for a brief period before clicking the next button\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Click on the dropdown\n",
    "dropdown = wait.until(EC.element_to_be_clickable((By.ID, \"select2-body_ddlreportemp-container\")))\n",
    "dropdown.click()\n",
    "\n",
    "\n",
    "# Locate and click the \"Breakage\" option\n",
    "manager_option = wait.until(EC.element_to_be_clickable((By.XPATH, \"//li[contains(text(), 'Breakage')]\")))\n",
    "manager_option.click()\n",
    "\n",
    "# Find the \"Load\" button element\n",
    "load_button_1 = wait.until(EC.element_to_be_clickable((By.ID, \"Btnsearchid\")))\n",
    "\n",
    "# Click the \"Load\" button\n",
    "load_button_1.click()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "html_code_br = driver.page_source\n",
    "# Parse the HTML content\n",
    "\n",
    "soup_br = BeautifulSoup(html_code_br, 'html.parser')\n",
    "\n",
    "# Find the table element\n",
    "table_br = soup_br.find('table', {'id': 'usertable'})  # Replace 'tblline' with the actual ID of your table\n",
    "\n",
    "# Extract the table headers\n",
    "headers_br = [th.text.strip() for th in table_br.select('thead th')]\n",
    "\n",
    "# Extract the table rows\n",
    "rows_br = []\n",
    "for tr in table_br.select('tbody tr'):\n",
    "    row_br = [td.text.strip() for td in tr.select('td')]\n",
    "    rows_br.append(row_br)\n",
    "\n",
    "# Check if there are any rows in the footer of the table\n",
    "footer_rows_br = table_br.select('tfoot tr')\n",
    "if footer_rows_br:\n",
    "    # Extract the additional rows from the footer\n",
    "    additional_rows_br = []\n",
    "    for tr in footer_rows_br:\n",
    "        row_br = [td.text.strip() for td in tr.select('td')]\n",
    "        additional_rows_br.append(row_br)\n",
    "\n",
    "    # Add the additional rows at the end of the extracted rows\n",
    "    rows_br.extend(additional_rows_br)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df_br = pd.DataFrame(rows_br, columns=headers_br)\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "df_br.to_excel('breakage_C.xlsx', index=False)\n",
    "\n",
    "\n",
    "\n",
    "time.sleep(10)\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb3df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f6e23c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"http://192.168.168.216/premierJute/myPanel/trendcomparison\")\n",
    "\n",
    "wait = WebDriverWait(driver, 60)\n",
    "\n",
    "\n",
    "# Find the button using the provided relative XPath\n",
    "button = wait.until(EC.visibility_of_element_located((By.XPATH, \"//label[@id='lnum']\")))\n",
    "\n",
    "driver.set_window_size(1200, 800)\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "# Click on the button\n",
    "button.click()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "html_code = driver.page_source\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(html_code, 'html.parser')\n",
    "\n",
    "# Find the table element\n",
    "table = soup.find('table', {'id': 'tblline'})\n",
    "\n",
    "# Extract the table headers\n",
    "headers = [th.text for th in table.find('thead').find_all('th')]\n",
    "\n",
    "# Extract the table rows\n",
    "rows = []\n",
    "for tr in table.find('tbody').find_all('tr'):\n",
    "    row = [td.text.strip() for td in tr.find_all('td')]\n",
    "    if row:\n",
    "        rows.append(row)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "df.to_excel('kg.xlsx', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Execute JavaScript to unselect the checkbox\n",
    "driver.execute_script(\"document.querySelector(\\\"label[for='chk40']\\\").previousElementSibling.checked = false;\")\n",
    "\n",
    "\n",
    "# Execute JavaScript to select the first checkbox\n",
    "driver.execute_script(\"document.querySelector(\\\"label[for='chk9']\\\").click();\")\n",
    "\n",
    "# Wait for a brief moment\n",
    "time.sleep(2)\n",
    "\n",
    "# Execute JavaScript to select the second checkbox\n",
    "driver.execute_script(\"document.querySelector(\\\"label[for='chk69']\\\").click();\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract the HTML content of the page\n",
    "html_code_2 = driver.page_source\n",
    "\n",
    "# Parse the HTML code with BeautifulSoup\n",
    "soup_2 = BeautifulSoup(html_code_2, 'html.parser')\n",
    "\n",
    "# Find the table element\n",
    "table_2 = soup_2.find('table', {'id': 'tblline'})\n",
    "\n",
    "# Extract the table headers\n",
    "headers_2 = [th.text.strip() for th in table_2.select('thead th')]\n",
    "\n",
    "# Extract the table rows\n",
    "rows_2 = []\n",
    "for tr in table_2.select('tbody tr'):\n",
    "    row_2 = [td.text.strip() for td in tr.select('td')]\n",
    "    rows_2.append(row_2)\n",
    "\n",
    "# Check if there are any rows in the footer of the table\n",
    "footer_rows_2 = table_2.select('tfoot tr')\n",
    "if footer_rows_2:\n",
    "    # Extract the additional rows from the footer\n",
    "    additional_rows_2 = []\n",
    "    for tr in footer_rows_2:\n",
    "        row_2 = [td.text.strip() for td in tr.select('td')]\n",
    "        additional_rows_2.append(row_2)\n",
    "\n",
    "    # Add the additional rows at the end of the extracted rows\n",
    "    rows_2.extend(additional_rows_2)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df_2 = pd.DataFrame(rows_2, columns=headers_2)\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "df_2.to_excel('trendline_1.xlsx', index=False)\n",
    "\n",
    "\n",
    "# Execute JavaScript to unselect the first checkbox\n",
    "driver.execute_script(\"document.querySelector(\\\"label[for='chk9']\\\").click();\")\n",
    "\n",
    "# Wait for a brief moment\n",
    "time.sleep(2)\n",
    "\n",
    "# Execute JavaScript to unselect the second checkbox\n",
    "driver.execute_script(\"document.querySelector(\\\"label[for='chk69']\\\").click();\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Execute JavaScript to select the first checkbox\n",
    "driver.execute_script(\"document.querySelector(\\\"label[for='chk15']\\\").click();\")\n",
    "\n",
    "# Wait for a brief moment\n",
    "time.sleep(2)\n",
    "\n",
    "# Execute JavaScript to select the second checkbox\n",
    "driver.execute_script(\"document.querySelector(\\\"label[for='chk32']\\\").click();\")\n",
    "\n",
    "\n",
    "# Extract the HTML content of the page\n",
    "html_code_2 = driver.page_source\n",
    "\n",
    "# Parse the HTML code with BeautifulSoup\n",
    "soup_2 = BeautifulSoup(html_code_2, 'html.parser')\n",
    "\n",
    "# Find the table element\n",
    "table_2 = soup_2.find('table', {'id': 'tblline'})\n",
    "\n",
    "# Extract the table headers\n",
    "headers_2 = [th.text.strip() for th in table_2.select('thead th')]\n",
    "\n",
    "# Extract the table rows\n",
    "rows_2 = []\n",
    "for tr in table_2.select('tbody tr'):\n",
    "    row_2 = [td.text.strip() for td in tr.select('td')]\n",
    "    rows_2.append(row_2)\n",
    "\n",
    "# Check if there are any rows in the footer of the table\n",
    "footer_rows_2 = table_2.select('tfoot tr')\n",
    "if footer_rows_2:\n",
    "    # Extract the additional rows from the footer\n",
    "    additional_rows_2 = []\n",
    "    for tr in footer_rows_2:\n",
    "        row_2 = [td.text.strip() for td in tr.select('td')]\n",
    "        additional_rows_2.append(row_2)\n",
    "\n",
    "    # Add the additional rows at the end of the extracted rows\n",
    "    rows_2.extend(additional_rows_2)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df_2 = pd.DataFrame(rows_2, columns=headers_2)\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "df_2.to_excel('trendline_2.xlsx', index=False)\n",
    "\n",
    "\n",
    "# Function to convert columns to numeric except for specified columns\n",
    "def convert_columns_to_numeric(df, exclude_columns):\n",
    "    converted_df = df.copy()\n",
    "    for column in df.columns:\n",
    "        if column not in exclude_columns:\n",
    "            converted_df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    return converted_df\n",
    "\n",
    "# List of Excel file names\n",
    "excel_files = [\"trendline_1.xlsx\",\"trendline_2.xlsx\"]\n",
    "\n",
    "\n",
    "\n",
    "# List of columns to exclude from conversion\n",
    "exclude_columns = [\"Period\"]\n",
    "\n",
    "# Process each Excel file\n",
    "for file in excel_files:\n",
    "    # Read Excel file\n",
    "    df = pd.read_excel(file)\n",
    "    \n",
    "    # Convert columns to numeric\n",
    "    converted_df = convert_columns_to_numeric(df, exclude_columns)\n",
    "    \n",
    "    # Save the modified DataFrame back to the same Excel file\n",
    "    converted_df.to_excel(file, index=False)\n",
    "\n",
    "\n",
    "time.sleep(10)\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d9942bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manager\n",
    "\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the webpage\n",
    "driver.get(\"http://192.168.168.216/premierJute/myPanel/userdefinedreports\")\n",
    "\n",
    "wait = WebDriverWait(driver, 20)\n",
    "\n",
    "# Find and click the 'From Date' input field to open the calendar\n",
    "from_date_input = wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='txtfromdate']\")))\n",
    "from_date_input.click()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find and click the 'From S1' button\n",
    "from_s1_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@id='btnFrmS1']\")))\n",
    "from_s1_button.click()\n",
    "\n",
    "# Find and click the 'To Date' input field to open the calendar\n",
    "to_date_input = wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='txttodate']\")))\n",
    "to_date_input.click()\n",
    "\n",
    "\n",
    "to_s3_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@id='btnToS1']\")))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", to_s3_button)\n",
    "driver.execute_script(\"arguments[0].click();\", to_s3_button)\n",
    "\n",
    "# Click on the dropdown\n",
    "dropdown = wait.until(EC.element_to_be_clickable((By.ID, \"select2-body_ddlreportemp-container\")))\n",
    "dropdown.click()\n",
    "\n",
    "\n",
    "# Locate and click the \"Manager\" option\n",
    "manager_option = wait.until(EC.element_to_be_clickable((By.XPATH, \"//li[contains(text(), 'Manager')]\")))\n",
    "manager_option.click()\n",
    "\n",
    "# Click on the \"Load\" button\n",
    "load_button = wait.until(EC.element_to_be_clickable((By.ID, \"Btnsearchid\")))\n",
    "load_button.click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Click on the desired button\n",
    "# Find and click the dropdown button\n",
    "# dropdown_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@data-toggle='dropdown']\")))\n",
    "# dropdown_button.click()\n",
    "\n",
    "# # Click the third button\n",
    "# third_button.click()\n",
    "\n",
    "# # Find the dropdown menu\n",
    "# dropdown_menu = wait.until(EC.presence_of_element_located((By.ID, \"drpsorting\")))\n",
    "\n",
    "# # Find and click the button with the text \"Excel\"\n",
    "# excel_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//a[normalize-space()='Excel']\")))\n",
    "\n",
    "# excel_button.click()\n",
    "\n",
    "# Extract the HTML content of the page\n",
    "\n",
    "html_code_2 = driver.page_source\n",
    "# Parse the HTML content\n",
    "\n",
    "soup_2 = BeautifulSoup(html_code_2, 'html.parser')\n",
    "\n",
    "# Find the table element\n",
    "table_2 = soup_2.find('table', {'id': 'usertable'})  # Replace 'tblline' with the actual ID of your table\n",
    "\n",
    "# Extract the table headers\n",
    "headers_2 = [th.text.strip() for th in table_2.select('thead th')]\n",
    "\n",
    "# Extract the table rows\n",
    "rows_2 = []\n",
    "for tr in table_2.select('tbody tr'):\n",
    "    row_2 = [td.text.strip() for td in tr.select('td')]\n",
    "    rows_2.append(row_2)\n",
    "\n",
    "# Check if there are any rows in the footer of the table\n",
    "footer_rows_2 = table_2.select('tfoot tr')\n",
    "if footer_rows_2:\n",
    "    # Extract the additional rows from the footer\n",
    "    additional_rows_2 = []\n",
    "    for tr in footer_rows_2:\n",
    "        row_2 = [td.text.strip() for td in tr.select('td')]\n",
    "        additional_rows_2.append(row_2)\n",
    "\n",
    "    # Add the additional rows at the end of the extracted rows\n",
    "    rows_2.extend(additional_rows_2)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df_2 = pd.DataFrame(rows_2, columns=headers_2)\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "df_2.to_excel('manager_A.xlsx', index=False)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a993774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manager\n",
    "\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the webpage\n",
    "driver.get(\"http://192.168.168.216/premierJute/myPanel/userdefinedreports\")\n",
    "\n",
    "wait = WebDriverWait(driver, 20)\n",
    "\n",
    "# Find and click the 'From Date' input field to open the calendar\n",
    "from_date_input = wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='txtfromdate']\")))\n",
    "from_date_input.click()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find and click the 'From S1' button\n",
    "from_s1_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@id='btnFrmS2']\")))\n",
    "from_s1_button.click()\n",
    "\n",
    "# Find and click the 'To Date' input field to open the calendar\n",
    "to_date_input = wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='txttodate']\")))\n",
    "to_date_input.click()\n",
    "\n",
    "\n",
    "to_s3_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@id='btnToS2']\")))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", to_s3_button)\n",
    "driver.execute_script(\"arguments[0].click();\", to_s3_button)\n",
    "\n",
    "# Click on the dropdown\n",
    "dropdown = wait.until(EC.element_to_be_clickable((By.ID, \"select2-body_ddlreportemp-container\")))\n",
    "dropdown.click()\n",
    "\n",
    "\n",
    "# Locate and click the \"Manager\" option\n",
    "manager_option = wait.until(EC.element_to_be_clickable((By.XPATH, \"//li[contains(text(), 'Manager')]\")))\n",
    "manager_option.click()\n",
    "\n",
    "# Click on the \"Load\" button\n",
    "load_button = wait.until(EC.element_to_be_clickable((By.ID, \"Btnsearchid\")))\n",
    "load_button.click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Click on the desired button\n",
    "# Find and click the dropdown button\n",
    "# dropdown_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@data-toggle='dropdown']\")))\n",
    "# dropdown_button.click()\n",
    "\n",
    "# # Click the third button\n",
    "# third_button.click()\n",
    "\n",
    "# # Find the dropdown menu\n",
    "# dropdown_menu = wait.until(EC.presence_of_element_located((By.ID, \"drpsorting\")))\n",
    "\n",
    "# # Find and click the button with the text \"Excel\"\n",
    "# excel_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//a[normalize-space()='Excel']\")))\n",
    "\n",
    "# excel_button.click()\n",
    "\n",
    "# Extract the HTML content of the page\n",
    "\n",
    "html_code_2 = driver.page_source\n",
    "# Parse the HTML content\n",
    "\n",
    "soup_2 = BeautifulSoup(html_code_2, 'html.parser')\n",
    "\n",
    "# Find the table element\n",
    "table_2 = soup_2.find('table', {'id': 'usertable'})  # Replace 'tblline' with the actual ID of your table\n",
    "\n",
    "# Extract the table headers\n",
    "headers_2 = [th.text.strip() for th in table_2.select('thead th')]\n",
    "\n",
    "# Extract the table rows\n",
    "rows_2 = []\n",
    "for tr in table_2.select('tbody tr'):\n",
    "    row_2 = [td.text.strip() for td in tr.select('td')]\n",
    "    rows_2.append(row_2)\n",
    "\n",
    "# Check if there are any rows in the footer of the table\n",
    "footer_rows_2 = table_2.select('tfoot tr')\n",
    "if footer_rows_2:\n",
    "    # Extract the additional rows from the footer\n",
    "    additional_rows_2 = []\n",
    "    for tr in footer_rows_2:\n",
    "        row_2 = [td.text.strip() for td in tr.select('td')]\n",
    "        additional_rows_2.append(row_2)\n",
    "\n",
    "    # Add the additional rows at the end of the extracted rows\n",
    "    rows_2.extend(additional_rows_2)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df_2 = pd.DataFrame(rows_2, columns=headers_2)\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "df_2.to_excel('manager_B.xlsx', index=False)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2c6d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manager\n",
    "\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the webpage\n",
    "driver.get(\"http://192.168.168.216/premierJute/myPanel/userdefinedreports\")\n",
    "\n",
    "wait = WebDriverWait(driver, 20)\n",
    "\n",
    "# Find and click the 'From Date' input field to open the calendar\n",
    "from_date_input = wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='txtfromdate']\")))\n",
    "from_date_input.click()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find and click the 'From S1' button\n",
    "from_s1_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@id='btnFrmS3']\")))\n",
    "from_s1_button.click()\n",
    "\n",
    "# Find and click the 'To Date' input field to open the calendar\n",
    "to_date_input = wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='txttodate']\")))\n",
    "to_date_input.click()\n",
    "\n",
    "\n",
    "to_s3_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@id='btnToS3']\")))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", to_s3_button)\n",
    "driver.execute_script(\"arguments[0].click();\", to_s3_button)\n",
    "\n",
    "# Click on the dropdown\n",
    "dropdown = wait.until(EC.element_to_be_clickable((By.ID, \"select2-body_ddlreportemp-container\")))\n",
    "dropdown.click()\n",
    "\n",
    "\n",
    "# Locate and click the \"Manager\" option\n",
    "manager_option = wait.until(EC.element_to_be_clickable((By.XPATH, \"//li[contains(text(), 'Manager')]\")))\n",
    "manager_option.click()\n",
    "\n",
    "# Click on the \"Load\" button\n",
    "load_button = wait.until(EC.element_to_be_clickable((By.ID, \"Btnsearchid\")))\n",
    "load_button.click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Click on the desired button\n",
    "# Find and click the dropdown button\n",
    "# dropdown_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@data-toggle='dropdown']\")))\n",
    "# dropdown_button.click()\n",
    "\n",
    "# # Click the third button\n",
    "# third_button.click()\n",
    "\n",
    "# # Find the dropdown menu\n",
    "# dropdown_menu = wait.until(EC.presence_of_element_located((By.ID, \"drpsorting\")))\n",
    "\n",
    "# # Find and click the button with the text \"Excel\"\n",
    "# excel_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//a[normalize-space()='Excel']\")))\n",
    "\n",
    "# excel_button.click()\n",
    "\n",
    "# Extract the HTML content of the page\n",
    "\n",
    "html_code_2 = driver.page_source\n",
    "# Parse the HTML content\n",
    "\n",
    "soup_2 = BeautifulSoup(html_code_2, 'html.parser')\n",
    "\n",
    "# Find the table element\n",
    "table_2 = soup_2.find('table', {'id': 'usertable'})  # Replace 'tblline' with the actual ID of your table\n",
    "\n",
    "# Extract the table headers\n",
    "headers_2 = [th.text.strip() for th in table_2.select('thead th')]\n",
    "\n",
    "# Extract the table rows\n",
    "rows_2 = []\n",
    "for tr in table_2.select('tbody tr'):\n",
    "    row_2 = [td.text.strip() for td in tr.select('td')]\n",
    "    rows_2.append(row_2)\n",
    "\n",
    "# Check if there are any rows in the footer of the table\n",
    "footer_rows_2 = table_2.select('tfoot tr')\n",
    "if footer_rows_2:\n",
    "    # Extract the additional rows from the footer\n",
    "    additional_rows_2 = []\n",
    "    for tr in footer_rows_2:\n",
    "        row_2 = [td.text.strip() for td in tr.select('td')]\n",
    "        additional_rows_2.append(row_2)\n",
    "\n",
    "    # Add the additional rows at the end of the extracted rows\n",
    "    rows_2.extend(additional_rows_2)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df_2 = pd.DataFrame(rows_2, columns=headers_2)\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "df_2.to_excel('manager_C.xlsx', index=False)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd5ef970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for breakage\n",
    "import pandas as pd\n",
    "\n",
    "# Function to convert columns to numeric except for specified columns\n",
    "def convert_columns_to_numeric(df, exclude_columns):\n",
    "    converted_df = df.copy()\n",
    "    for column in df.columns:\n",
    "        if column not in exclude_columns:\n",
    "            converted_df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    return converted_df\n",
    "\n",
    "# List of Excel file names\n",
    "excel_files = [\"breakage_A.xlsx\", \"breakage_B.xlsx\", \"breakage_C.xlsx\"]\n",
    "\n",
    "# List of columns to exclude from conversion\n",
    "exclude_columns = [\"Count and Material\"]\n",
    "\n",
    "# Process each Excel file\n",
    "for file in excel_files:\n",
    "    # Read Excel file\n",
    "    df = pd.read_excel(file)\n",
    "    \n",
    "    # Convert columns to numeric\n",
    "    converted_df = convert_columns_to_numeric(df, exclude_columns)\n",
    "    \n",
    "    # Save the modified DataFrame back to the same Excel file\n",
    "    converted_df.to_excel(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2137e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe474f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for manager\n",
    "import pandas as pd\n",
    "\n",
    "# Function to convert columns to numeric except for specified columns\n",
    "def convert_columns_to_numeric(df, exclude_columns):\n",
    "    converted_df = df.copy()\n",
    "    for column in df.columns:\n",
    "        if column not in exclude_columns:\n",
    "            converted_df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    return converted_df\n",
    "\n",
    "# List of Excel file names\n",
    "excel_files = [\"manager_A.xlsx\", \"manager_B.xlsx\", \"manager_C.xlsx\"]\n",
    "\n",
    "# List of columns to exclude from conversion\n",
    "exclude_columns = [\"Count and Material\"]\n",
    "\n",
    "# Process each Excel file\n",
    "for file in excel_files:\n",
    "    # Read Excel file\n",
    "    df = pd.read_excel(file)\n",
    "    \n",
    "    # Convert columns to numeric\n",
    "    converted_df = convert_columns_to_numeric(df, exclude_columns)\n",
    "    \n",
    "    # Save the modified DataFrame back to the same Excel file\n",
    "    converted_df.to_excel(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65a5812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Set up the Selenium webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "wait = WebDriverWait(driver, 20)\n",
    "# Navigate to the web page\n",
    "driver.get(\"http://192.168.168.216/premierJute/myPanel/userdefinedreports\")\n",
    "\n",
    "# Find and input the 'From Date'\n",
    "from_date_input = driver.find_element(By.XPATH, \"//input[@id='txtfromdate']\")\n",
    "from_date_input.click()\n",
    "\n",
    "# Click on the 'From' button\n",
    "from_button = driver.find_element(By.XPATH, \"//button[@id='btnFrmS1']\")\n",
    "from_button.click()\n",
    "\n",
    "# Find and input the 'To Date'\n",
    "to_date_input = driver.find_element(By.XPATH, \"//input[@id='txttodate']\")\n",
    "to_date_input.click()\n",
    "\n",
    "# Click on the 'To' button\n",
    "to_button = driver.find_element(By.XPATH, \"//button[@id='btnToS1']\")\n",
    "to_button.click()\n",
    "\n",
    "\n",
    "# Find and click on the dropdown\n",
    "dropdown = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, \"//span[@id='select2-body_ddlreportemp-container']\"))\n",
    ")\n",
    "dropdown.click()\n",
    "\n",
    "time.sleep(5)\n",
    "# Locate and click the \"Manager\" option\n",
    "kg_option = wait.until(EC.element_to_be_clickable((By.XPATH, \"//li[contains(text(), 'Kg')]\")))\n",
    "kg_option.click()\n",
    "\n",
    "\n",
    "# Click on the search button\n",
    "search_button = driver.find_element(By.XPATH, \"//button[@id='Btnsearchid']\")\n",
    "search_button.click()\n",
    "time.sleep(10)\n",
    "\n",
    "\n",
    "\n",
    "html_code_2 = driver.page_source\n",
    "# Parse the HTML content\n",
    "\n",
    "soup_2 = BeautifulSoup(html_code_2, 'html.parser')\n",
    "\n",
    "# Find the table element\n",
    "table_2 = soup_2.find('table', {'id': 'usertable'})  # Replace 'tblline' with the actual ID of your table\n",
    "\n",
    "# Extract the table headers\n",
    "headers_2 = [th.text.strip() for th in table_2.select('thead th')]\n",
    "\n",
    "# Extract the table rows\n",
    "rows_2 = []\n",
    "for tr in table_2.select('tbody tr'):\n",
    "    row_2 = [td.text.strip() for td in tr.select('td')]\n",
    "    rows_2.append(row_2)\n",
    "\n",
    "# Check if there are any rows in the footer of the table\n",
    "footer_rows_2 = table_2.select('tfoot tr')\n",
    "if footer_rows_2:\n",
    "    # Extract the additional rows from the footer\n",
    "    additional_rows_2 = []\n",
    "    for tr in footer_rows_2:\n",
    "        row_2 = [td.text.strip() for td in tr.select('td')]\n",
    "        additional_rows_2.append(row_2)\n",
    "\n",
    "    # Add the additional rows at the end of the extracted rows\n",
    "    rows_2.extend(additional_rows_2)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df_2 = pd.DataFrame(rows_2, columns=headers_2)\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "df_2.to_excel('Kg_A.xlsx', index=False)\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51d87ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Set up the Selenium webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "wait = WebDriverWait(driver, 20)\n",
    "# Navigate to the web page\n",
    "driver.get(\"http://192.168.168.216/premierJute/myPanel/userdefinedreports\")\n",
    "\n",
    "# Find and input the 'From Date'\n",
    "from_date_input = driver.find_element(By.XPATH, \"//input[@id='txtfromdate']\")\n",
    "from_date_input.click()\n",
    "\n",
    "# Click on the 'From' button\n",
    "from_button = driver.find_element(By.XPATH, \"//button[@id='btnFrmS2']\")\n",
    "from_button.click()\n",
    "\n",
    "# Find and input the 'To Date'\n",
    "to_date_input = driver.find_element(By.XPATH, \"//input[@id='txttodate']\")\n",
    "to_date_input.click()\n",
    "\n",
    "# Click on the 'To' button\n",
    "to_button = driver.find_element(By.XPATH, \"//button[@id='btnToS2']\")\n",
    "to_button.click()\n",
    "\n",
    "\n",
    "# Find and click on the dropdown\n",
    "dropdown = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, \"//span[@id='select2-body_ddlreportemp-container']\"))\n",
    ")\n",
    "dropdown.click()\n",
    "\n",
    "time.sleep(5)\n",
    "# Locate and click the \"Manager\" option\n",
    "kg_option = wait.until(EC.element_to_be_clickable((By.XPATH, \"//li[contains(text(), 'Kg')]\")))\n",
    "kg_option.click()\n",
    "\n",
    "\n",
    "# Click on the search button\n",
    "search_button = driver.find_element(By.XPATH, \"//button[@id='Btnsearchid']\")\n",
    "search_button.click()\n",
    "time.sleep(10)\n",
    "\n",
    "\n",
    "\n",
    "html_code_2 = driver.page_source\n",
    "# Parse the HTML content\n",
    "\n",
    "soup_2 = BeautifulSoup(html_code_2, 'html.parser')\n",
    "\n",
    "# Find the table element\n",
    "table_2 = soup_2.find('table', {'id': 'usertable'})  # Replace 'tblline' with the actual ID of your table\n",
    "\n",
    "# Extract the table headers\n",
    "headers_2 = [th.text.strip() for th in table_2.select('thead th')]\n",
    "\n",
    "# Extract the table rows\n",
    "rows_2 = []\n",
    "for tr in table_2.select('tbody tr'):\n",
    "    row_2 = [td.text.strip() for td in tr.select('td')]\n",
    "    rows_2.append(row_2)\n",
    "\n",
    "# Check if there are any rows in the footer of the table\n",
    "footer_rows_2 = table_2.select('tfoot tr')\n",
    "if footer_rows_2:\n",
    "    # Extract the additional rows from the footer\n",
    "    additional_rows_2 = []\n",
    "    for tr in footer_rows_2:\n",
    "        row_2 = [td.text.strip() for td in tr.select('td')]\n",
    "        additional_rows_2.append(row_2)\n",
    "\n",
    "    # Add the additional rows at the end of the extracted rows\n",
    "    rows_2.extend(additional_rows_2)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df_2 = pd.DataFrame(rows_2, columns=headers_2)\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "df_2.to_excel('Kg_B.xlsx', index=False)\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d43b853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Set up the Selenium webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "wait = WebDriverWait(driver, 20)\n",
    "# Navigate to the web page\n",
    "driver.get(\"http://192.168.168.216/premierJute/myPanel/userdefinedreports\")\n",
    "\n",
    "# Find and input the 'From Date'\n",
    "from_date_input = driver.find_element(By.XPATH, \"//input[@id='txtfromdate']\")\n",
    "from_date_input.click()\n",
    "\n",
    "# Click on the 'From' button\n",
    "from_button = driver.find_element(By.XPATH, \"//button[@id='btnFrmS3']\")\n",
    "from_button.click()\n",
    "\n",
    "# Find and input the 'To Date'\n",
    "to_date_input = driver.find_element(By.XPATH, \"//input[@id='txttodate']\")\n",
    "to_date_input.click()\n",
    "\n",
    "# Click on the 'To' button\n",
    "to_button = driver.find_element(By.XPATH, \"//button[@id='btnToS3']\")\n",
    "to_button.click()\n",
    "\n",
    "\n",
    "# Find and click on the dropdown\n",
    "dropdown = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, \"//span[@id='select2-body_ddlreportemp-container']\"))\n",
    ")\n",
    "dropdown.click()\n",
    "\n",
    "time.sleep(5)\n",
    "# Locate and click the \"Manager\" option\n",
    "kg_option = wait.until(EC.element_to_be_clickable((By.XPATH, \"//li[contains(text(), 'Kg')]\")))\n",
    "kg_option.click()\n",
    "\n",
    "\n",
    "# Click on the search button\n",
    "search_button = driver.find_element(By.XPATH, \"//button[@id='Btnsearchid']\")\n",
    "search_button.click()\n",
    "time.sleep(10)\n",
    "\n",
    "\n",
    "\n",
    "html_code_2 = driver.page_source\n",
    "# Parse the HTML content\n",
    "\n",
    "soup_2 = BeautifulSoup(html_code_2, 'html.parser')\n",
    "\n",
    "# Find the table element\n",
    "table_2 = soup_2.find('table', {'id': 'usertable'})  # Replace 'tblline' with the actual ID of your table\n",
    "\n",
    "# Extract the table headers\n",
    "headers_2 = [th.text.strip() for th in table_2.select('thead th')]\n",
    "\n",
    "# Extract the table rows\n",
    "rows_2 = []\n",
    "for tr in table_2.select('tbody tr'):\n",
    "    row_2 = [td.text.strip() for td in tr.select('td')]\n",
    "    rows_2.append(row_2)\n",
    "\n",
    "# Check if there are any rows in the footer of the table\n",
    "footer_rows_2 = table_2.select('tfoot tr')\n",
    "if footer_rows_2:\n",
    "    # Extract the additional rows from the footer\n",
    "    additional_rows_2 = []\n",
    "    for tr in footer_rows_2:\n",
    "        row_2 = [td.text.strip() for td in tr.select('td')]\n",
    "        additional_rows_2.append(row_2)\n",
    "\n",
    "    # Add the additional rows at the end of the extracted rows\n",
    "    rows_2.extend(additional_rows_2)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df_2 = pd.DataFrame(rows_2, columns=headers_2)\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "df_2.to_excel('Kg_C.xlsx', index=False)\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b37f9dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for manager\n",
    "import pandas as pd\n",
    "\n",
    "# Function to convert columns to numeric except for specified columns\n",
    "def convert_columns_to_numeric(df, exclude_columns):\n",
    "    converted_df = df.copy()\n",
    "    for column in df.columns:\n",
    "        if column not in exclude_columns:\n",
    "            converted_df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    return converted_df\n",
    "\n",
    "# List of Excel file names\n",
    "excel_files = [\"Kg_A.xlsx\", \"Kg_B.xlsx\", \"Kg_C.xlsx\"]\n",
    "\n",
    "# List of columns to exclude from conversion\n",
    "exclude_columns = [\"Count and Material\"]\n",
    "\n",
    "# Process each Excel file\n",
    "for file in excel_files:\n",
    "    # Read Excel file\n",
    "    df = pd.read_excel(file)\n",
    "    \n",
    "    # Convert columns to numeric\n",
    "    converted_df = convert_columns_to_numeric(df, exclude_columns)\n",
    "    \n",
    "    # Save the modified DataFrame back to the same Excel file\n",
    "    converted_df.to_excel(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15a33c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b5168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba55b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "002b2a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'AEF %' not found in the file: manager_B.xlsx. Skipping filtering.\n",
      "Column 'AEF %' not found in the file: manager_C.xlsx. Skipping filtering.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of Excel file names\n",
    "excel_files = [\"manager_A.xlsx\", \"manager_B.xlsx\", \"manager_C.xlsx\"]\n",
    "\n",
    "# Process each Excel file\n",
    "for file in excel_files:\n",
    "    # Read Excel file\n",
    "    df = pd.read_excel(file)\n",
    "    \n",
    "    # Check if \"AEF %\" column exists in the DataFrame\n",
    "    if \"AEF %\" in df.columns:\n",
    "        # Filter rows based on condition\n",
    "        df = df[df[\"AEF %\"] > 30]\n",
    "        \n",
    "        # Save the filtered DataFrame back to the same Excel file\n",
    "        df.to_excel(file, index=False)\n",
    "    else:\n",
    "        print(f\"Column 'AEF %' not found in the file: {file}. Skipping filtering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83474dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "manager_df_A=pd.read_excel(\"manager_A.xlsx\")\n",
    "\n",
    "running_frame=manager_df_A[\"AEF %\"][:-1].count() #C23\n",
    "avg_efficiency=manager_df_A[\"AEF %\"][:-1].mean()\n",
    "avg_efficiency_percentage = \"{:.2f}%\".format(avg_efficiency)#C24\n",
    "total_idle_spndl=manager_df_A[\"eb idle\"][:-1].sum() #C28\n",
    "total_doffing=manager_df_A[\"doffs\"][:-1].sum() #C30\n",
    "avg_min_doff=manager_df_A[\"min/doff\"][:-1].mean() #C31\n",
    "stop_minutes=manager_df_A[\"Total M/c Stop time\"][:-1].sum() #C32\n",
    "total_end_br=manager_df_A[\"eb total\"][:-1].sum() #C33\n",
    "                                     \n",
    "\n",
    "                                     # Load the workbook\n",
    "# Load the workbook\n",
    "workbook = load_workbook(\"UltimoJJ analysis report.xlsx\")\n",
    "\n",
    "# Select the desired sheet (e.g., Summary)\n",
    "sheet = workbook[\"Summary\"]\n",
    "\n",
    "# Write the data to specific cells\n",
    "sheet[\"C23\"] = running_frame\n",
    "sheet[\"C24\"] = avg_efficiency_percentage\n",
    "sheet[\"C28\"] = total_idle_spndl\n",
    "sheet[\"C30\"] = total_doffing\n",
    "sheet[\"C31\"] = avg_min_doff\n",
    "sheet[\"C32\"] = stop_minutes\n",
    "sheet[\"C33\"] = total_end_br\n",
    "\n",
    "\n",
    "manager_df_B=pd.read_excel(\"manager_B.xlsx\")\n",
    "\n",
    "running_frame=manager_df_B[\"AEF %\"][:-1].count() \n",
    "avg_efficiency=manager_df_B[\"AEF %\"][:-1].mean()\n",
    "avg_efficiency_percentage = \"{:.2f}%\".format(avg_efficiency)\n",
    "total_idle_spndl=manager_df_B[\"eb idle\"][:-1].sum() \n",
    "total_doffing=manager_df_B[\"doffs\"][:-1].sum() \n",
    "avg_min_doff=manager_df_B[\"min/doff\"][:-1].mean() \n",
    "stop_minutes=manager_df_B[\"Total M/c Stop time\"][:-1].sum()\n",
    "total_end_br=manager_df_B[\"eb total\"][:-1].sum() \n",
    "\n",
    "\n",
    "# Write the data to specific cells\n",
    "sheet[\"F23\"] = running_frame\n",
    "sheet[\"F24\"] = avg_efficiency_percentage\n",
    "sheet[\"F28\"] = total_idle_spndl\n",
    "sheet[\"F30\"] = total_doffing\n",
    "sheet[\"F31\"] = avg_min_doff\n",
    "sheet[\"F32\"] = stop_minutes\n",
    "sheet[\"F33\"] = total_end_br\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "manager_df_C=pd.read_excel(\"manager_C.xlsx\")\n",
    "\n",
    "running_frame=manager_df_C[\"AEF %\"][:-1].count() \n",
    "avg_efficiency=manager_df_C[\"AEF %\"][:-1].mean()\n",
    "avg_efficiency_percentage = \"{:.2f}%\".format(avg_efficiency)\n",
    "total_idle_spndl=manager_df_C[\"eb idle\"][:-1].sum() \n",
    "total_doffing=manager_df_C[\"doffs\"][:-1].sum() \n",
    "avg_min_doff=manager_df_C[\"min/doff\"][:-1].mean() \n",
    "stop_minutes=manager_df_C[\"Total M/c Stop time\"][:-1].sum() \n",
    "total_end_br=manager_df_C[\"eb total\"][:-1].sum() \n",
    "\n",
    "\n",
    "# Write the data to specific cells\n",
    "sheet[\"I23\"] = running_frame\n",
    "sheet[\"I24\"] = avg_efficiency_percentage\n",
    "sheet[\"I28\"] = total_idle_spndl\n",
    "sheet[\"I30\"] = total_doffing\n",
    "sheet[\"I31\"] = avg_min_doff\n",
    "sheet[\"I32\"] = stop_minutes\n",
    "sheet[\"I33\"] = total_end_br\n",
    "\n",
    "# Save the modified workbook\n",
    "workbook.save(\"UltimoJJ analysis report.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32cde277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of file names\n",
    "file_names = ['breakage_A.xlsx', 'breakage_B.xlsx', 'breakage_C.xlsx']\n",
    "\n",
    "# Initialize an empty DataFrame to store the concatenated data\n",
    "concatenated_data = pd.DataFrame()\n",
    "\n",
    "# Iterate over the file names and read each Excel file\n",
    "for file_name in file_names:\n",
    "    # Read the Excel file into a DataFrame\n",
    "    df = pd.read_excel(file_name)\n",
    "    \n",
    "    # Exclude the last row of the DataFrame\n",
    "    df = df.iloc[:-1]\n",
    "    \n",
    "    # Concatenate the data to the existing DataFrame\n",
    "    concatenated_data = pd.concat([concatenated_data, df], ignore_index=True)\n",
    "\n",
    "# Save the concatenated data to a new Excel file\n",
    "concatenated_data.to_excel('breakage.xlsx', index=False)\n",
    "\n",
    "\n",
    "# Read the concatenated data from the Excel file\n",
    "concatenated_data = pd.read_excel('breakage.xlsx')\n",
    "\n",
    "# Drop blank rows from the DataFrame\n",
    "concatenated_data = concatenated_data.dropna(how='all')\n",
    "\n",
    "# Save the updated data to the Excel file\n",
    "concatenated_data.to_excel('breakage.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39bae6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the data from the breakage.xlsx file\n",
    "# data = pd.read_excel('breakage.xlsx')\n",
    "\n",
    "# # Group the data by M/c No. and calculate the average of AEF % and the sum of Total Piecings\n",
    "# grouped_data = data.groupby('M/c No.').agg({'AEF %': 'mean', 'Total Piecings': 'sum'}).reset_index()\n",
    "\n",
    "# # Save the grouped data to the convt_break.xlsx file\n",
    "# grouped_data.to_excel('convt_break.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a118d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the previous data from convt_kg.xlsx if it exists\n",
    "try:\n",
    "    previous_data = pd.read_excel('convt_break.xlsx')\n",
    "except FileNotFoundError:\n",
    "    previous_data = pd.DataFrame()\n",
    "\n",
    "# Read the new data from kg.xlsx\n",
    "new_data = pd.read_excel('breakage.xlsx')\n",
    "\n",
    "# Concatenate previous data and new data\n",
    "combined_data = pd.concat([previous_data, new_data], ignore_index=True)\n",
    "\n",
    "# Group the combined data by M/c No. and calculate the mean of AEF %\n",
    "grouped_data = combined_data.groupby('M/c No.').agg({'AEF %': 'mean', 'Total Piecings': 'sum'}).reset_index()\n",
    "\n",
    "# Save the grouped data to convt_kg.xlsx\n",
    "grouped_data.to_excel('convt_break.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db360a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of file names\n",
    "file_names = ['Kg_A.xlsx', 'Kg_B.xlsx', 'Kg_C.xlsx']\n",
    "\n",
    "# Initialize an empty DataFrame to store the concatenated data\n",
    "concatenated_data = pd.DataFrame()\n",
    "\n",
    "# Iterate over the file names and read each Excel file\n",
    "for file_name in file_names:\n",
    "    # Read the Excel file into a DataFrame\n",
    "    df = pd.read_excel(file_name)\n",
    "    \n",
    "    # Exclude the last row of the DataFrame\n",
    "    df = df.iloc[:-1]\n",
    "    \n",
    "    # Concatenate the data to the existing DataFrame\n",
    "    concatenated_data = pd.concat([concatenated_data, df], ignore_index=True)\n",
    "\n",
    "# Save the concatenated data to a new Excel file\n",
    "concatenated_data.to_excel('kg.xlsx', index=False)\n",
    "\n",
    "\n",
    "# Read the concatenated data from the Excel file\n",
    "concatenated_data = pd.read_excel('kg.xlsx')\n",
    "\n",
    "# Drop blank rows from the DataFrame\n",
    "concatenated_data = concatenated_data.dropna(how='all')\n",
    "\n",
    "# Save the updated data to the Excel file\n",
    "concatenated_data.to_excel('kg.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93dae6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the breakage.xlsx file\n",
    "data = pd.read_excel('kg.xlsx')\n",
    "\n",
    "# Group the data by M/c No. and calculate the average of AEF % and the sum of Total Piecings\n",
    "grouped_data = data.groupby('M/c No.').agg({'AEF %': 'mean'}).reset_index()\n",
    "\n",
    "# Save the grouped data to the convt_break.xlsx file\n",
    "grouped_data.to_excel('convt_kg.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b1e7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the previous data from convt_kg.xlsx if it exists\n",
    "try:\n",
    "    previous_data = pd.read_excel('convt_kg.xlsx')\n",
    "except FileNotFoundError:\n",
    "    previous_data = pd.DataFrame()\n",
    "\n",
    "# Read the new data from kg.xlsx\n",
    "new_data = pd.read_excel('kg.xlsx')\n",
    "\n",
    "# Concatenate previous data and new data\n",
    "combined_data = pd.concat([previous_data, new_data], ignore_index=True)\n",
    "\n",
    "# Group the combined data by M/c No. and calculate the mean of AEF %\n",
    "grouped_data = combined_data.groupby('M/c No.').agg({'AEF %': 'mean'}).reset_index()\n",
    "\n",
    "# Save the grouped data to convt_kg.xlsx\n",
    "grouped_data.to_excel('convt_kg.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
